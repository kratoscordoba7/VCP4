<h1 align="center">üåü Pr√°ctica 4 - Visi√≥n por Computador (Curso 2024/2025)</h1>

<img align="left" width="150" height="150" src="https://github.com/AlejandroDavidArzolaSaavedra/Kata-Working-With-Sqlite/assets/90756437/f83020eb-76e4-4224-87e4-ae2a2d370b05g"></a>
Se han completado todas las tareas solicitadas de la **Pr√°ctica 4** para la asignatura **Visi√≥n por Computador**. Reconocimiento de matr√≠culas

*Trabajo realizado por*:

[![GitHub](https://img.shields.io/badge/GitHub-Heliot%20J.%20Segura%20Gonzalez-purple?style=flat-square&logo=github)](https://github.com/kratoscordoba7)

[![GitHub](https://img.shields.io/badge/GitHub-Alejandro%20D.%20Arzola%20Saavedra%20-darkblue?style=flat-square&logo=github)](https://github.com/AlejandroDavidArzolaSaavedra)

## üõ†Ô∏è Librer√≠as Utilizadas

[![OpenCV](https://img.shields.io/badge/OpenCV-%23FF8C00.svg?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)
[![Pytesseract](https://img.shields.io/badge/Pytesseract-%2300A3E0.svg?style=for-the-badge&logo=pytorch&logoColor=white)](https://github.com/madmaze/pytesseract)
[![YOLO](https://img.shields.io/badge/YOLO-%231f77b4.svg?style=for-the-badge&logo=python&logoColor=white)](https://github.com/ultralytics/yolov5)
[![EasyOCR](https://img.shields.io/badge/EasyOCR-%23009639.svg?style=for-the-badge&logo=python&logoColor=white)](https://github.com/JaidedAI/EasyOCR)
[![Math](https://img.shields.io/badge/Math-%23013243.svg?style=for-the-badge&logo=python&logoColor=white)](https://docs.python.org/3/library/math.html)
[![Regex](https://img.shields.io/badge/Regex-%234C4CFF.svg?style=for-the-badge&logo=python&logoColor=white)](https://docs.python.org/3/library/re.html)



---
## üöÄ C√≥mo empezar

Para comenzar con el proyecto, sigue estos pasos:

> [!NOTE]  
> Debes de situarte en un envioronment configurado como se defini√≥ en el cuaderno de la pr√°ctica 1  de [otsedom](https://github.com/otsedom/otsedom.github.io/blob/main/VC/P1/README.md#111-comandos-basicos-de-anaconda) y el de la pr√°ctica 4 de [otsedom](https://github.com/otsedom/otsedom.github.io/blob/main/VC/P4/README.md).

### Paso 1: Abrir VSCode y situarse en el directorio:
   
   `C:\Users\TuNombreDeUsuario\anaconda3\envs\VCP4
   
### Paso 2: Clonar y trabajar en el proyecto localmente (VS Code)
1. **Clona el repositorio**: Ejecuta el siguiente comando en tu terminal para clonar el repositorio:
   ```bash
   git clone https://github.com/kratoscordoba7/VCP4.git
   ```
2. Una vez clonado, todos los archivos han de estar situado en el envioronment del paso 1

### Paso 3: Abrir Anaconda prompt y activar el envioroment:
   ```bash
   conda activate NombreDeTuEnvironment
   ```
Tras estos pasos deber√≠a poder ejecutar el proyecto localmente

---

<h2 align="center">üìã Tareas</h2>

### Tarea 1 Reconocimiento de matr√≠culas

Para la entrega de esta pr√°ctica, la tarea consiste en desarrollar un prototipo que procese uno (v√≠deo ejemplo proporcionado) o varios v√≠deos (incluyendo v√≠deos de cosecha propia):

detecte y siga las personas y veh√≠culos presentes
detecte y lea las matr√≠culas de los veh√≠culos presentes
cuente el total de cada clase
vuelque a disco un v√≠deo que visualice los resultados
genere un archivo csv con el resultado de la detecci√≥n y seguimiento. Se sugiere un formato con al menos los siguientes campos:

```python
  fotograma, tipo_objeto, confianza, identificador_tracking, x1, y1, x2, y2, matr√≠cula_en_su_caso, confianza, mx1,my1,mx2,my2, texto_matricula
````

La entrega del cuaderno o cuadernos se hace efectiva a trav√©s del campus virtual por medio de un enlace github. Adem√°s del archivo README, debe incluirse el resultado del v√≠deo proporcionado como test (o enlace al mismo), y el correspondiente archivo csv. En el caso de entrenarse alg√∫n detector, por ejemplo de matr√≠culas, debe proporcionarse acceso al conjunto de datos.

Se considerar√°n extras:

- Determine el flujo de personas y veh√≠culos en el v√≠deo de test en distintas direcciones (veh√≠culos que dejan la imagen por la derecha, por la izquierda, etc.)

Participar en 
 - Evaluar dos alternativas para la detecci√≥n de matr√≠culas: basada en YOLO, y basada en contornos.
 - Anonimizar a las personas y veh√≠culos presentes en un v√≠deo.


En primer lugar se opt√≥ por realizar un entrenamiento mediante la plataforma RoboFlow para entrenar un modelo que distinga tanto coches como matr√≠culas. Las m√©tricas de entrenamiento se pueden ver en las siguientes figuras:
<div style="text-align: center;" align="center"> <img src="img/metrica_overflow.png" height="400px" width="500px" alt="Metricas roboflow 1"> </div>
A continuaci√≥n se van a explicar la diferentes gr√°ficas de la fotograf√≠a:

- **El mAP** es una m√©trica utilizada en tareas de detecci√≥n de objetos para evaluar la precisi√≥n del modelo en diferentes umbrales de intersecci√≥n sobre la uni√≥n (IoU). En este caso, se muestran dos variantes del mAP:
  - **mAP** (l√≠nea en p√∫rpura oscuro): mide la precisi√≥n media en varios umbrales de IoU (desde 0.5 hasta 0.95).
  - **mAP@50:95** (l√≠nea en p√∫rpura claro): eval√∫a el rendimiento a un umbral espec√≠fico de IoU (0.50), lo que suele ser m√°s permisivo.

En el √°mbito del mAP, se puede concluir que a lo largo de las √©pocas, se observa una tendencia general de mejora en ambas m√©tricas. El mAP se estabiliza cerca de 0.9, lo que indica un buen rendimiento del modelo.
El mAP@50:95 es m√°s bajo pero sigue una tendencia ascendente, estabiliz√°ndose en alrededor de 0.6, lo que muestra que el modelo sigue mejorando, aunque con menor precisi√≥n a mayores umbrales.

Por otro lado,  la p√©rdida asociada a la predicci√≥n de las cajas delimitadoras (bounding boxes), que representan los objetos en una imagen. La p√©rdida se refiere a la diferencia entre las predicciones del modelo y las cajas reales (ground truth).
- Inicialmente, la Box Loss es alta (alrededor de 2.2), lo que significa que el modelo comet√≠a muchos errores en la localizaci√≥n de los objetos.
- A medida que las √©pocas avanzan, la p√©rdida disminuye y se estabiliza en torno a 1.0, indicando que el modelo mejora en su capacidad para predecir las posiciones de los objetos.

La p√©rdida de clasificaci√≥n, que mide la capacidad del modelo para identificar correctamente a qu√© clase pertenece un objeto detectado.
- Al principio, la p√©rdida es alta, alrededor de 3.5, lo que indica un mal rendimiento en la clasificaci√≥n de los objetos.
- A medida que el modelo entrena, la p√©rdida disminuye y se estabiliza entre 0.5 y 1.0, lo que significa que el modelo se vuelve mucho m√°s preciso al identificar las clases de los objetos.

En el gr√°fico de la derecha, se muestra la p√©rdida de objetos, que est√° relacionada con la capacidad del modelo para detectar correctamente la presencia de objetos en una imagen.
- Inicialmente, la p√©rdida es alta (alrededor de 4.5), indicando que el modelo fallaba con frecuencia al detectar la presencia de objetos.
- Aunque hay algunos picos pronunciados en las primeras √©pocas, la p√©rdida disminuye y se estabiliza cerca de 1.5, lo que sugiere que el modelo mejora en su habilidad para identificar objetos con mayor precisi√≥n.

Este modelo, en cuanto a m√©tricas se refiere, present√≥ un rendimiento atractivo. Existen dos problemas por parte de Roboflow por lo cual terminamos descartando dicha opci√≥n:
- La primera de ellas es que roboflow exige el usao de la API para poder usar el modelo, el modus operandi es el siguiente:
   - Primero pides a la API de roboflow mediante una API key que te d√© el modelo (no lo tienes en local, lo tienes mediante una conexi√≥n a la API).
   - Para obtener la predicci√≥n de una imagen se tiene que requerir a la API para que cada frame por individual lo procese.
      - El gran problema de  esto es que un v√≠deo cualquiera de no mucha duraci√≥n se hace eterno para que termine.
- El segundo problema es que el resultado que se obtuvo por parte de este modelo no fue bueno, de hecho, se podr√≠a decir que fue bastante malo.

Aqu√≠ se presenta un ejemplo de su comportamiento:

<div style="text-align: center;" align="center"> <img src="img/roboflow.gif" alt="Veh√≠culo detectado con RoboFlow"> </div>

## Video de demostraci√≥n



Se puede apreciar que no distingue correctamente los dos coches, los trata como una sola unidad. Adem√°s, a pesar de que se ve que detecta la matr√≠cula en este ejemplo en otros muchos no lo hac√≠a

En vista a lo sucedido, decidimos cambiar de estrategia y, como nos comentaron los docentes de la materia, decidimos seguir el siguiente algoritmo:
- Usar un clasificador de personas y coches. Cuando este est√© clasificando algo se lo pasamos a un clasificador √∫nicamente de matr√≠culas que tambi√©n decidimos entrenar mediante roboflow y estos fueron las m√©tricas obtenidas

<div style="text-align: center;" align="center"> <img src="img/metrica_2_overflwo.png" height="400px" width="500px" alt="Veh√≠culo detectado con YOLO"> </div>

Los resultados de este segundo modelo fueron similares a los del primero, a pesar de que este se enfocaba √∫nicamente en la clasificaci√≥n de matr√≠culas. Aunque las m√©tricas iniciales indicaban un modelo prometedor, los resultados finales fueron decepcionantes: el modelo tuvo dificultades para manejar tanto im√°genes con ruido (por ejemplo, im√°genes con interferencias o baja calidad) como sin √©l. Consideramos que el problema no se deb√≠a al dataset, ya que cont√°bamos con m√°s de 290 im√°genes, muchas de ellas modificadas para evitar el sobreajuste. De igual manera, al tener que llamar a la API de roboflow por cada frame el v√≠deo se ralentzi√≥ demasiado. 

En esta situaci√≥n, se opt√≥ por entrenar un modelo con el mismo dataset que se us√≥ con roboflow pero en local clonando el git de ultralytics para YOLO v8 y los resultados que se obtuvieron son:

<div class="image-grid">
</div>

<table>
    <td width="50%">
        <img src="img/F1_curve.png" alt="Imagen 1">
    </td>
    <td width="50%">
       <img src="img/PR_curve.png" alt="Imagen 2">
    </td>
</table>
<table>
    <td width="50%">
       <img src="img/P_curve.png" alt="Imagen 3"> 
    </td>
   <td width="50%">
       <img src="img/R_curve.png" alt="Imagen 4">
    </td>
</table>

- El recall se mantiene muy cerca de 1.0 para la mayor√≠a de los valores de confianza (desde 0 hasta aproximadamente 0.8), lo que indica que el modelo es capaz de detectar la mayor√≠a de los ejemplos positivos (matr√≠culas en este caso) en una amplia gama de niveles de confianza.
- A partir de un umbral de confianza cercano a 0.8, el recall comienza a decaer r√°pidamente, lo que indica que el modelo se vuelve m√°s estricto al aceptar predicciones positivas y, por lo tanto, comienza a perder algunos ejemplos positivos. Este comportamiento es esperado porque a medida que se aumenta el umbral de confianza, el modelo es m√°s cauteloso, prefiriendo no hacer una predicci√≥n antes que cometer errores.
- La curva indica que tanto la precisi√≥n como el recall est√°n cerca de 1.0 para la clase de "matr√≠cula", lo cual es un excelente resultado. Esto significa que el modelo tiene una capacidad sobresaliente para identificar matr√≠culas sin cometer muchos errores.
- El hecho de que la curva est√© casi en la esquina superior derecha sugiere que el modelo mantiene altos niveles de precisi√≥n incluso cuando tambi√©n tiene un alto recall, lo cual es lo ideal en cualquier modelo de detecci√≥n.

Se obtuvieron buenos resultados y a la hora de probarlo no era malo, se comport√≥ decentemente. Sin embargo, decimos quedarnos con dos modelos preentrenado que encontramos:
- Uno detecta personas y coches
- Otro detecta matr√≠culas

A continuaci√≥n, presentamos los resultados obtenidos:

<div style="text-align: center;" align="center"> <img src="img/coche_dectectado.png" height="400px" width="500px" alt="Veh√≠culo detectado con YOLO"> </div>

Para la anonimizaci√≥n de las personas se us√≥ el siguiente c√≥digo:

```python
# Anonimizamos las personas
if class_name == "person":

  # Aplicamos un desenfoque a la regi√≥n de la persona
  person_roi = img[y1:y2, x1:x2]
  blurred_person = cv2.GaussianBlur(person_roi, (51, 51), 0)
  img[y1:y2, x1:x2] = blurred_person

  # Detectamos la direcci√≥n de la persona
  direction = "left" if x2 < img.shape[1] // 2 else "right"
  directions[direction] += 1
```

Primer caso: Utilizamos YOLO para detectar el veh√≠culo, indicando el nivel de confianza con el que se ha logrado la identificaci√≥n.

<div style="text-align: center;" align="center"> <img src="img/yolo_matricula.png" width="400px" alt="YOLO detectando matr√≠cula"> </div>

Segundo caso: El sistema detecta las matr√≠culas de los veh√≠culos y nos proporciona la confianza asociada a cada detecci√≥n.

<div style="text-align: center;" align="center"> <img src="img/consola_matricula_detectada.png" width="400px" alt="Consola mostrando matr√≠cula detectada"> </div>

En esta imagen, correspondiente al segundo caso, se muestra en consola la matr√≠cula detectada por el sistema.

<div style="text-align: center;" align="center"> <img src="img/matricula_coche.png" alt="Matr√≠cula reconocida"> </div>

Tercer caso: Aqu√≠ se ilustra c√≥mo el sistema ha detectado y reconocido correctamente la matr√≠cula, mostr√°ndola en pantalla.

<div style="text-align: center;" align="center"> <img src="img/person_anonymous.png"  height="325px" width="500px" alt="Detecci√≥n de veh√≠culos y personas con anonimato"> </div>

Cuarto caso: Adem√°s de detectar los veh√≠culos, el sistema tambi√©n identifica a las personas y aplica un filtro que oculta su identidad, respetando su privacidad.

Se a√±aden dos v√≠deos del funcionamiento de nuestra propuesta final con pytesseract y con easyOCR. A priori pueden parecer id√©nticos pero ha de fijarse en el lector de la matr√≠cula:

<table>
   <td width="50%">
   <h3 align="center">Video Pytesseract</h3>
   <div align="center">
   <img src="img/output_video_pytesseract.gif" alt="Veh√≠culo detectado con RoboFlow">
   </div>                                                                       
   </td>
   <td width="50%">
   <h3 align="center">Video EasyOCR</h3>
   <div align="center">
   <img src="img/output_video_easyOCR.gif" alt="Veh√≠culo detectado con RoboFlow">
   </div>                                                                       
   </td>
</table>


Para el almacenamiento en el archivo csv se aplica las siguientes instrucciones:
```python
# Inicializamos el archivo CSV
csv_file = open("media/detected_objects_easyOCR.csv", mode='w', newline='')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(['frame', 'object_type', 'confidence', 'tracking_id', 'x1', 'y1', 'x2', 'y2', 'license_plate', 'plate_confidence', 'direction'])
...More code...
# Escribimos los datos en el archivo CSV
csv_writer.writerow([frame_count, class_name, confidence, tracking_id, x1, y1, x2, y2, plate_text, confidence, direction]
```
Por √∫ltimo, para el almacenaje del v√≠deo el c√≥digo implicado es:
```python
# Inicializamos el objeto para grabar el video con los resultados
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('media/output_video_easyOCR.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))
```
En la funci√≥n cv2.VideoWriter() los par√°metros son:
- El nombre del archivo
- fourcc, es un c√≥digo de cuatros caracteres que se usa para comprimir los frames. Por ejemplo, VideoWriter::fourcc('M','J','P','G') es un "motion-jpeg" codificador.
- FPS (fotograma por segundo)
- El tama√±o de los frame del v√≠deo

Para obtener mejores conclusiones y analizar los resultados de forma m√°s precisa, decidimos utilizar la biblioteca Chart.js, ampliamente popular en la creaci√≥n de gr√°ficos interactivos.

<div align="center"><img src="https://github.com/user-attachments/assets/93b6fea5-f70e-401f-9a89-0dbf8b3fa893"></div>

Como podemos observar, el modelo muestra mayor confianza en la detecci√≥n de veh√≠culos que en la de personas. Esto puede deberse a varios factores, como la iluminaci√≥n, los faros de los coches o la rapidez de movimiento de las personas.

En cuanto al conteo, el sistema ha detectado m√°s personas que coches, aunque estas cifras no necesariamente coinciden con el n√∫mero real de personas o veh√≠culos que pasan por el video. En realidad, representan la cantidad de veces que el sistema ha identificado estos objetos, lo cual indica una mayor estabilidad en la detecci√≥n de los coches.

<div align="center"><img src="https://github.com/user-attachments/assets/aee4fe12-76f1-4fe8-b869-8a88eb024e28"></div>

En este caso, podemos ver la diferencia entre EasyOCR y PyTesseract y la eficiencia de EasyOCR para el reconocimiento de matr√≠culas.


> [!IMPORTANT]  
> Los archivos presentados aqu√≠ son una modificaci√≥n de los archivos originales de [otsedom](https://github.com/otsedom/otsedom.github.io/tree/main/VC).



---

## üìö Bibliograf√≠a

1. [Why does rotation of an image using cv2.getRotationMatrix2D and cv2.warpAffine result in clipping?](https://stackoverflow.com/questions/62370352/why-does-rotation-of-an-image-using-cv2-getrotationmatrix2d-and-cv2-warpaffine-r)
2. [OpenCV Histogram Equalization](https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html)
3. [Python Pytesseract image to string can't read text in image](https://stackoverflow.com/questions/59496336/python-pytesseract-image-to-string-cant-read-text-in-image)
4. [Tesseract OCR fails to correctly recognize text in vehicle license plates](https://stackoverflow.com/questions/78899657/tesseract-ocr-fails-to-correctly-recognize-text-in-vehicle-license-plates)
5. [OpenCV Thresholding](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)
6. [Reconocimiento de matr√≠culas vehiculares con OpenCV y Pytesseract OCR en Python](https://omes-va.com/reconocimiento-de-matriculas-vehiculares-opencv-pytesseract-ocr-python/)
7. [Documentaci√≥n ofical de OpenCV para la generaci√≥n del v√≠deo](https://docs.opencv.org/4.x/dd/d9e/classcv_1_1VideoWriter.html#ad59c61d8881ba2b2da22cff5487465b5)
8. [Documentaci√≥n oficial de python para escritura de ficheros .csv](https://docs.python.org/3/library/csv.html)

---

**Universidad de Las Palmas de Gran Canaria**  

EII - Grado de Ingenier√≠a Inform√°tica  
Obra bajo licencia de Creative Commons Reconocimiento - No Comercial 4.0 Internacional

---
